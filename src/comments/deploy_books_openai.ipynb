{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"../../data/books-sample/comments.csv\"\n",
    "NORMALIZED_DATA_PATH = \"../../data/books-sample/normalized_comments.csv\"\n",
    "COLLECTION_NAME = \"comments_books_openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hazm import *\n",
    "\n",
    "normalizer = Normalizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_comments = pd.read_csv(DATA_PATH)\n",
    "# drop duplicates\n",
    "sample_comments = sample_comments.drop_duplicates(subset=[\"id\"])\n",
    "# drop non string comments\n",
    "sample_comments = sample_comments.dropna(subset=[\"body\"])\n",
    "sample_comments[\"normalized_body\"] = sample_comments[\"body\"].apply(\n",
    "    lambda x: normalizer.normalize(x)\n",
    ")\n",
    "\n",
    "# save to file\n",
    "sample_comments.to_csv(NORMALIZED_DATA_PATH, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sample_comments = pd.read_csv(NORMALIZED_DATA_PATH)\n",
    "\n",
    "# create a map out of product_id to comments\n",
    "product_comments_map = {}\n",
    "for _, row in sample_comments.iterrows():\n",
    "    product_id = row[\"product_id\"]\n",
    "    comment = row\n",
    "    if product_id not in product_comments_map:\n",
    "        product_comments_map[product_id] = []\n",
    "    product_comments_map[product_id].append(comment)\n",
    "\n",
    "# sort product_comments_map by length of comments\n",
    "product_comments_map = dict(\n",
    "    sorted(product_comments_map.items(), key=lambda item: len(item[1]), reverse=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products with more than 5 comments: 1000\n"
     ]
    }
   ],
   "source": [
    "# filter product_comments_map for more than 5 comments\n",
    "product_comments_map = {\n",
    "    product_id: comments\n",
    "    for product_id, comments in product_comments_map.items()\n",
    "    if len(comments) > 5\n",
    "}\n",
    "\n",
    "print(f\"Number of products with more than 5 comments: {len(product_comments_map)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def call_generate_api(comments):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": \"llama3\",\n",
    "        \"prompt\": f\"\"\"I have a list of comments that need to be summarized. Each comment contains various points and details that are crucial for understanding the overall feedback. The summary should be detailed enough to enable effective semantic search for the most relevant results later on.\n",
    "\n",
    "                1. The main idea or topic of the comments.\n",
    "                2. Specific points and details mentioned.\n",
    "                3. Any notable examples or anecdotes provided.\n",
    "\n",
    "                Adhere to these guidelines:\n",
    "                1. Craft a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness.\n",
    "                2. Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects.\n",
    "                3. Rely strictly on the provided text, without including external information.\n",
    "                4. Your response should be in a single paragraph and contains only the summary of the comments.\n",
    "                5. You should not engage in a conversation you should just produce a summary of the comments.\n",
    "                6. produce the summery in a single paragraph containing all the details and points mentioned in the comments.\n",
    "\n",
    "\n",
    "                COMMENTS: {comments}\"\"\",\n",
    "        \"stream\": False,\n",
    "    }\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "    response = requests.post(url, data=json.dumps(payload), headers=headers)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "    else:\n",
    "        return {\n",
    "            \"error\": f\"Request failed with status code {response.status_code}\",\n",
    "            \"details\": response.text,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "OPENAI_BASE_URL = os.getenv(\"OPENAI_BASE_URL\")\n",
    "\n",
    "\n",
    "openai_client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "\n",
    "\n",
    "def get_summary(comments):\n",
    "    system_prompt = \"You are a helpful assistant. You should not engage in a conversation with the user. Your response should be in persian language.\"\n",
    "\n",
    "    prompt = f\"\"\"I have a list of comments that need to be summarized. Each comment contains various points and details that are crucial for understanding the overall feedback. The summary should be detailed enough to enable effective semantic search for the most relevant results later on.\n",
    "\n",
    "                1. The main idea or topic of the comments.\n",
    "                2. Specific points and details mentioned.\n",
    "                3. Any notable examples or anecdotes provided.\n",
    "\n",
    "                Adhere to these guidelines:\n",
    "                1. Craft a summary that is detailed, thorough, in-depth, and complex, while maintaining clarity and conciseness.\n",
    "                2. Incorporate main ideas and essential information, eliminating extraneous language and focusing on critical aspects.\n",
    "                3. Rely strictly on the provided text, without including external information.\n",
    "                4. Your response should be in persian language. (زبان فارسی)\n",
    "                5. Your response should be in a single paragraph and contains only the summary of the comments.\n",
    "\n",
    "\n",
    "                COMMENTS: {comments}\"\"\"\n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_comments_summary_map = {}\n",
    "\n",
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n",
    "\n",
    "\n",
    "for item in product_comments_map.items():\n",
    "    print(\n",
    "        f\"Remaining products: {len(product_comments_map) - len(product_comments_summary_map)}\"\n",
    "    )\n",
    "    product_id = item[0]\n",
    "    product_comments = item[1]\n",
    "    product_comments_body = [comment[\"body\"] for comment in product_comments]\n",
    "\n",
    "    encoded = encoding.encode(str(product_comments_body))\n",
    "    chunk_size = 1\n",
    "    if len(encoded) > 15000:\n",
    "        chunk_size = 2\n",
    "        while True:\n",
    "            # compute the encoding for each chunk\n",
    "            for i in range(0, chunk_size):\n",
    "                chunk = product_comments_body[\n",
    "                    i\n",
    "                    * len(product_comments_body)\n",
    "                    // chunk_size : (i + 1)\n",
    "                    * len(product_comments_body)\n",
    "                    // chunk_size\n",
    "                ]\n",
    "                encoded = encoding.encode(str(chunk))\n",
    "                if len(encoded) > 15000:\n",
    "                    chunk_size *= 2\n",
    "                    continue\n",
    "            break\n",
    "\n",
    "    # for each chunk of comments, get the summary\n",
    "    for i in range(0, chunk_size):\n",
    "        chunk = product_comments_body[\n",
    "            i\n",
    "            * len(product_comments_body)\n",
    "            // chunk_size : (i + 1)\n",
    "            * len(product_comments_body)\n",
    "            // chunk_size\n",
    "        ]\n",
    "        summary = get_summary(chunk)\n",
    "        print(summary)\n",
    "        if product_id not in product_comments_summary_map:\n",
    "            product_comments_summary_map[product_id] = []\n",
    "        product_comments_summary_map[product_id].append(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "\n",
    "load_dotenv(\".env\")\n",
    "\n",
    "CHROMA_SERVER_AUTH_CREDENTIALS = os.getenv(\"CHROMA_SERVER_AUTH_CREDENTIALS\")\n",
    "\n",
    "client = chromadb.HttpClient(\n",
    "    host=\"https://chroma.liara.run\",\n",
    "    settings=Settings(\n",
    "        chroma_client_auth_provider=\"chromadb.auth.token_authn.TokenAuthClientProvider\",\n",
    "        chroma_client_auth_credentials=CHROMA_SERVER_AUTH_CREDENTIALS,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.create_collection(name=COLLECTION_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "ids = []\n",
    "metadatas = []\n",
    "embeddings = []\n",
    "\n",
    "for row in product_comments_summary_map.items():\n",
    "    print(f\"Remaining products: {len(product_comments_summary_map) - len(ids)}\")\n",
    "    id = row[0]\n",
    "    comments = row[1]\n",
    "    aggregated_comments = \" \".join(comments)\n",
    "    embedding = (\n",
    "        openai_client.embeddings.create(\n",
    "            input=aggregated_comments, model=\"text-embedding-3-small\"\n",
    "        )\n",
    "        .data[0]\n",
    "        .embedding\n",
    "    )\n",
    "    ids.append(id)\n",
    "    documents.append(aggregated_comments)\n",
    "    embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_collection = client.get_collection(name=COLLECTION_NAME)\n",
    "# convert ids to string\n",
    "ids = [str(id) for id in ids]\n",
    "metadatas = [{\"product_id\": id} for id in ids]\n",
    "\n",
    "comments_collection.upsert(\n",
    "    documents=documents,\n",
    "    ids=ids,\n",
    "    embeddings=embeddings,\n",
    "    metadatas=metadatas,\n",
    ")\n",
    "\n",
    "print(comments_collection.peek(limit=5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
